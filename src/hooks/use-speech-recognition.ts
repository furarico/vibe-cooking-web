import { useCallback, useRef, useState } from 'react';

interface SpeechRecognitionConstructor {
  new (): SpeechRecognition;
}

interface SpeechRecognition extends EventTarget {
  lang: string;
  continuous: boolean;
  interimResults: boolean;
  maxAlternatives: number;
  start(): void;
  stop(): void;
  abort(): void;
  onstart: ((this: SpeechRecognition, ev: Event) => void) | null;
  onresult:
    | ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => void)
    | null;
  onerror:
    | ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => void)
    | null;
  onend: ((this: SpeechRecognition, ev: Event) => void) | null;
}

interface SpeechRecognitionEvent extends Event {
  resultIndex: number;
  results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
}

interface SpeechRecognitionResultList {
  length: number;
  [index: number]: SpeechRecognitionResult;
}

interface SpeechRecognitionResult {
  length: number;
  isFinal: boolean;
  [index: number]: SpeechRecognitionAlternative;
}

interface SpeechRecognitionAlternative {
  transcript: string;
  confidence: number;
}

declare global {
  interface Window {
    SpeechRecognition: SpeechRecognitionConstructor;
    webkitSpeechRecognition: SpeechRecognitionConstructor;
  }
}

type RecognitionStatus =
  | 'idle'
  | 'listening'
  | 'processing'
  | 'success'
  | 'error';

// ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥æ©Ÿèƒ½
const detectTriggerWords = (text: string) => {
  const nextKeywords = [
    'æ¬¡',
    'ã¤ã',
    'ãƒ„ã‚®',
    'æ¬¡ã®',
    'ã¤ãã®',
    'ãƒã‚¯ã‚¹ãƒˆ',
    'next',
    'é€²ã‚“ã§',
  ];
  const prevKeywords = [
    'å‰',
    'ã¾ãˆ',
    'ãƒã‚¨',
    'å‰ã®',
    'ã¾ãˆã®',
    'ãƒãƒƒã‚¯',
    'back',
    'æˆ»ã‚‹',
    'ã‚‚ã©ã‚‹',
    'ã‚‚ã©ã£ã¦',
    'æˆ»ã£ã¦',
  ];

  const normalizedText = text.toLowerCase();

  const hasNext = nextKeywords.some(keyword =>
    normalizedText.includes(keyword.toLowerCase())
  );

  const hasPrev = prevKeywords.some(keyword =>
    normalizedText.includes(keyword.toLowerCase())
  );

  return { hasNext, hasPrev };
};

interface UseSpeechRecognitionOptions {
  onNextTrigger?: () => void;
  onPrevTrigger?: () => void;
  showRecipeSteps?: boolean;
}

export const useSpeechRecognition = (
  options: UseSpeechRecognitionOptions = {}
) => {
  const { onNextTrigger, onPrevTrigger, showRecipeSteps = false } = options;

  // éŸ³å£°èªè­˜ã®çŠ¶æ…‹
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [isProcessing, setIsProcessing] = useState(false);
  const [status, setStatus] = useState<RecognitionStatus>('idle');
  const [statusMessage, setStatusMessage] = useState('');
  const [shouldRestart, setShouldRestart] = useState(false);
  const [isTranscriptCompleted, setIsTranscriptCompleted] = useState(false);
  const [triggerHistory, setTriggerHistory] = useState<string[]>([]);

  // refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  const startRecording = useCallback(async () => {
    try {
      // Web Speech APIã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹å ´åˆ
      if (
        'webkitSpeechRecognition' in window ||
        'SpeechRecognition' in window
      ) {
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();

        recognition.lang = 'ja-JP';
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.maxAlternatives = 1;

        recognition.onstart = () => {
          setIsRecording(true);
          setStatus('listening');
          setStatusMessage('éŸ³å£°ã‚’èã„ã¦ã„ã¾ã™...');
        };

        recognition.onresult = (event: SpeechRecognitionEvent) => {
          let finalText = '';
          let interimText = '';

          for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcriptPart = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalText += transcriptPart;
            } else {
              interimText += transcriptPart;
            }
          }

          // æ–°ã—ãç¢ºå®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
          if (finalText) {
            console.log('ğŸ¤ éŸ³å£°èªè­˜çµæœ:', finalText);
            console.log('ğŸ“± ç¾åœ¨ã®çŠ¶æ…‹ - showRecipeSteps:', showRecipeSteps);

            // ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥
            const { hasNext, hasPrev } = detectTriggerWords(finalText);
            console.log('ğŸ” ãƒˆãƒªã‚¬ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥çµæœ:', { hasNext, hasPrev });

            let triggerMessage = '';
            if (hasNext) {
              triggerMessage = 'ã€Œæ¬¡ã€ã‚’æ„ŸçŸ¥ã—ã¾ã—ãŸ';
              console.log('â¡ï¸ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ç§»å‹•ã—ã¾ã™');
              setTriggerHistory(prev => [
                ...prev,
                `${new Date().toLocaleTimeString()}: æ¬¡ãƒˆãƒªã‚¬ãƒ¼æ¤œçŸ¥ - "${finalText}"`,
              ]);
              // ãƒ¬ã‚·ãƒ”ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤ºä¸­ã®å ´åˆã¯æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
              if (showRecipeSteps && onNextTrigger) {
                console.log('ğŸ“ nextStep()ã‚’å®Ÿè¡Œã—ã¾ã™');
                setTimeout(() => onNextTrigger(), 0);
              }
            }
            if (hasPrev) {
              triggerMessage = 'ã€Œå‰ã€ã‚’æ„ŸçŸ¥ã—ã¾ã—ãŸ';
              console.log('â¬…ï¸ å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ç§»å‹•ã—ã¾ã™');
              setTriggerHistory(prev => [
                ...prev,
                `${new Date().toLocaleTimeString()}: å‰ãƒˆãƒªã‚¬ãƒ¼æ¤œçŸ¥ - "${finalText}"`,
              ]);
              // ãƒ¬ã‚·ãƒ”ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤ºä¸­ã®å ´åˆã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸
              if (showRecipeSteps && onPrevTrigger) {
                console.log('ğŸ“ prevStep()ã‚’å®Ÿè¡Œã—ã¾ã™');
                setTimeout(() => onPrevTrigger(), 0);
              }
            }

            // å‰å›ã®æ–‡å­—èµ·ã“ã—ãŒå®Œäº†ã—ã¦ã„ãŸå ´åˆã¯ç½®ãæ›ãˆã€ãã†ã§ãªã‘ã‚Œã°è¿½åŠ 
            if (isTranscriptCompleted) {
              setTranscript(finalText);
              setIsTranscriptCompleted(false);
            } else {
              setTranscript(prev => prev + finalText);
            }

            // ç¢ºå®šå¾Œã€çŸ­æ™‚é–“successã‚’è¡¨ç¤ºã—ã¦ã‹ã‚‰listeningã«æˆ»ã‚‹
            setStatus('success');
            setStatusMessage(triggerMessage || 'æ–‡å­—èµ·ã“ã—å®Œäº†');
            setIsTranscriptCompleted(true);

            setTimeout(() => {
              if (shouldRestart && isRecording) {
                setStatus('listening');
                setStatusMessage('éŸ³å£°ã‚’èã„ã¦ã„ã¾ã™...');
              }
            }, 500);
          }

          // ä¸­é–“çµæœã‚’æ›´æ–°
          setInterimTranscript(interimText);

          // ä¸­é–“çµæœãŒã‚ã‚‹å ´åˆã¯å‡¦ç†ä¸­çŠ¶æ…‹
          if (interimText && !finalText) {
            setStatus('processing');
            setStatusMessage('éŸ³å£°ã‚’èªè­˜ä¸­...');
          }
        };

        recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
          console.error('Speech recognition error:', event.error);

          // no-speechã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è‡ªå‹•çš„ã«å†é–‹ã‚’è©¦è¡Œ
          if (event.error === 'no-speech' && shouldRestart) {
            console.log('No speech detected, will restart...');
            setStatus('listening');
            setStatusMessage('éŸ³å£°ã‚’å¾…æ©Ÿä¸­...');
            return;
          }

          // ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯åœæ­¢
          setIsRecording(false);
          setShouldRestart(false);
          setStatus('error');
          setStatusMessage(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${event.error}`);

          // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
          if (restartTimeoutRef.current) {
            clearTimeout(restartTimeoutRef.current);
            restartTimeoutRef.current = null;
          }
        };

        recognition.onend = () => {
          console.log('Recognition ended, shouldRestart:', shouldRestart);
          setInterimTranscript('');

          // ã‚¨ãƒ©ãƒ¼ã§ãªã„å ´åˆã€ã‹ã¤shouldRestartãŒtrueã®å ´åˆã¯è‡ªå‹•çš„ã«å†é–‹
          if (shouldRestart && status !== 'error') {
            restartTimeoutRef.current = setTimeout(() => {
              if (shouldRestart && recognitionRef.current) {
                try {
                  console.log('Restarting recognition...');
                  recognitionRef.current.start();
                } catch (error) {
                  console.error('Failed to restart recognition:', error);
                  setIsRecording(false);
                  setShouldRestart(false);
                  setStatus('error');
                  setStatusMessage('éŸ³å£°èªè­˜ã®å†é–‹ã«å¤±æ•—ã—ã¾ã—ãŸ');
                }
              }
            }, 100);
          } else {
            setIsRecording(false);
            setShouldRestart(false);
            if (status !== 'error') {
              setStatus('idle');
              setStatusMessage('');
            }
          }
        };

        recognitionRef.current = recognition;
        setShouldRestart(true);
        setIsTranscriptCompleted(false);
        recognition.start();
        return;
      }

      // Web Speech APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯MediaRecorderã‚’ä½¿ç”¨
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];

      mediaRecorder.ondataavailable = event => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(chunksRef.current, { type: 'audio/wav' });
        await transcribeAudio(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.start();
      setIsRecording(true);
    } catch (error) {
      console.error('Error accessing microphone:', error);
      alert('ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚');
    }
  }, [
    shouldRestart,
    isRecording,
    status,
    isTranscriptCompleted,
    showRecipeSteps,
    onNextTrigger,
    onPrevTrigger,
  ]);

  const stopRecording = useCallback(() => {
    setShouldRestart(false);

    // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }

    // Web Speech APIã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆ
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      setInterimTranscript('');
      return;
    }

    // MediaRecorderã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆ
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
      setIsProcessing(true);
    }
  }, [isRecording]);

  const transcribeAudio = useCallback(async (audioBlob: Blob) => {
    try {
      // Web Speech APIã‚’ä½¿ç”¨ã—ãŸéŸ³å£°èªè­˜
      if (
        'webkitSpeechRecognition' in window ||
        'SpeechRecognition' in window
      ) {
        const SpeechRecognition =
          window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();

        recognition.lang = 'ja-JP';
        recognition.continuous = false;
        recognition.interimResults = false;

        return new Promise((resolve, reject) => {
          recognition.onresult = (event: SpeechRecognitionEvent) => {
            const transcript = event.results[0][0].transcript;
            setTranscript(transcript);
            setIsProcessing(false);
            resolve(transcript);
          };

          recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
            console.error('Speech recognition error:', event.error);
            setIsProcessing(false);
            reject(new Error('éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ' + event.error));
          };

          recognition.start();
        });
      } else {
        // Web Speech APIãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰APIã‚’ä½¿ç”¨
        const formData = new FormData();
        formData.append('audio', audioBlob, 'audio.wav');

        const response = await fetch('/api/voice-cooking', {
          method: 'POST',
          body: formData,
        });

        if (!response.ok) {
          throw new Error('éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸ');
        }

        const data = await response.json();
        setTranscript(data.transcript);
      }
    } catch (error) {
      console.error('Error transcribing audio:', error);
      alert('éŸ³å£°ã®æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
    } finally {
      setIsProcessing(false);
    }
  }, []);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    setStatus('idle');
    setStatusMessage('');
    setShouldRestart(false);
    setIsTranscriptCompleted(false);

    // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ã‚¯ãƒªã‚¢
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }
  }, []);

  const clearTriggerHistory = useCallback(() => {
    setTriggerHistory([]);
  }, []);

  return {
    // çŠ¶æ…‹
    isRecording,
    transcript,
    interimTranscript,
    isProcessing,
    status,
    statusMessage,
    triggerHistory,

    // ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    startRecording,
    stopRecording,
    clearTranscript,
    clearTriggerHistory,
  };
};
